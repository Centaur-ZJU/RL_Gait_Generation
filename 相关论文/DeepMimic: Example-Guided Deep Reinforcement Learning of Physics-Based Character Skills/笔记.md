现有的动作生成严重依赖于手工设计  
为模拟人物创作动作非常困难  

第二章 Related Work 中，Reinforcement learning 部分有相关论文可以参考，如：
1. High-Dimensional Continuous Control Using Generalized Advantage Estimation：
   使用梯度策略在两足机器人上训练跑步和从地上站起来的功能，
2. 

5.1 的最后讲到，使用 PD 控制代替直接输出力矩，可以去除底层如阻尼、反馈等细节的影响，提高性能和学习速度

第 6 章 中提到 RL 的探索问题，作者提出改变 episode 的初始和结束状态的分布很有帮助

主要起作用的功能是两点：
1. RSI(随机状态初始化)：每次环境重置时，使用捕捉到的动作数据或人工设计的动作数据作为初始姿态，使机器人直接得到相关的经验，从而大大减小实际搜索空间
2. ES(提前终止)：在机器人的状态陷入某种无意义的情形时(如摔倒在地)，提前终止该次 episode，使得模型不用学习无意义的动作

