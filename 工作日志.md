## 2020.5.10 星期日

新装了ubuntu20.04 LTS  
安装了 spinningup  
新系统的软件及设置

## 2020.5.11 星期一

修改 /home/chk/spinningup/spinup/user_config.py 文件  
将 spinningup 训练数据默认保存路径为 "/home/chk/文档/Centaur/data"  
`DEFAULT_DATA_DIR = "/home/chk/文档/Centaur/data"`  

安装了 pybullet：  
`pip3 install pybullet`  

pybullet GUI (21.482s) 与 DIRECT (21.082s) 模式在时间上没有差别  
其中 GUI 还有图像界面出现  

目前想法是用 pybullet 注册 gym 环境，然后用 spinningup 直接跑  
存在以下问题：  
1. spinningup 的命令行模式无法直接运行 pybullet_gym 环境  
   + ~~下面两种方法并没有解决训练问题，每次训练时都会提前截断轨迹，而没有正常进行训练，应该是因为 pybullet 的问题导致的~~  
     接下来去找 pybullet 自带的环境源码看看情况，再尝试自己编写 pybullet+gym 的环境时弄懂了，该问题已解决  
   + ~~这个问题暂时用 `gym_env.py` 解决了~~  
     数据图表可以用 plot 画出来  
     但是测试出现问题: `pybullet.error: Couldn't restore state.`   
     已解决，这是因为 test_policy 使用了保存下来的环境数据，使用时没有重新 connect pybullet 引擎，只要重新 make 一下，再用 run_policy 测试即可，详见 `my_test_policy.py`
   + 另一种方法: 在 `gym/envs/__init__.py` 6-11 行注册了 pybullet_gym 的环境，注册后即可用命令行进行训练  
2. 看 pybullet 自带的 gym 环境非常复杂，如果自己写环境也这么复杂的话怕是有点困难  
3. 参数的调整将会是个复杂且长期的工作  

## 2020.5.12-2020.5.17

~~pybullet_gym 毕竟不是纯粹的 gym 环境，适配的问题较大，决定暂时搁置~~  
接下来尝试自己写使用 pybullet 的 gym 环境：  
- [x] 写一个没有 pybullet 的简单环境并注册到 gym 中进行训练  
    - [x] 使用 gym [官方教程](https://github.com/openai/gym/blob/master/docs/creating-environments.md)注册一个空环境  
    - [x] 完成，训练、测试、画图均正常，见 myGymEnv/， 主要参考了 gym-pendulum 源码;~~参考 [gym-soccer](https://github.com/openai/gym-soccer)、[OpenAI Gym构建自定义强化学习环境](https://blog.csdn.net/extremebingo/article/details/80867486)、[强化学习：gym库的二次开发学习](https://www.jianshu.com/p/2062b6bef5ea) 等文章和~~ gym-pendulum 源码简单修改环境，使的 spinup 能够在其上正常训练，不被异常中断  
- [ ] 完成使用 pybullet 为引擎、能用 spinup 训练的 gym 环境  
    - [ ] 给 myGymEnv 加入 pybullet，测试 init,step,render 等功能, 参考了 pybullet_envs/env_bases.py  
        - [x] 加入 pybullet 引擎并能够渲染，spinup test_policy 功能似乎并不是重新构造环境，而是从保存下来的数据中恢复环境，然后测试策略，这导致了测试时会有 pybullet 未连接的情形，因为恢复的环境中其 physicsClientId 非负，表示它连接着引擎，而实际上训练时的引擎已经被关闭了，测试时并没有连接引擎，由此而报错  
        - [x] ~~自己写一个测试程序，或~~解决 spinup test_policy 的异常， 见`my_test_policy_py`  
        - [x] 用 ppo 在 pybullet-gym 中的 HumanoidFlagrunBulletEnv-v0 训练一晚，看看训练效果如何
              一个 epoch 大约用时 2s，总共进行 6 种 共 18 次实验(3个随机种子，3种网络，2种激活函数),设定总 epochs=1000, 预计用时9小时  
              开始于 2020.5.13-20:52，2020.5.14-8：43 时训练已结束，结果如下：  
              1. 根据画图结果来看，模型性能稳定提升；
              2. 32-32 的网络优于其它网络，接下来舍弃较大的网络，换更小的网络看看情况；
              3. 激活函数上，relu 要略微优于 tanh；
              4. 训练过程中的信息显示，训练轨迹总是被提前截断，导致本来计划的 4000 steps_per_epoch 通常只能运行数十个 steps，接下来调整参数再做一次训练  
              5. 最后测试结果表现很差，轨迹被提前截断的情况非常严重，需要查看环境源码寻找环境终止条件
        - [x] 继续用 ppo 在 pybullet 中的 HumanoidFlagrunBulletEnv-v0 训练 100 个 epochs，找找合适的参数  
              1. 用四个网络 [16,16],[32,32],[16,],[32,] 训练，激活函数都用 relu，结果表示 [16,16] 效果较好  
              2. 使用不同的 target_kl值 [0.01, 0.03, 0.05, 0.07, 0.09]，0.07 略微好些  
              3. 使用不同的 clip_ratio 值 [0.1, 0.15, 0.2, 0.25, 0.3]，0.3 明显优于其它参数，多做一次调整：[0.3,0.5,0.7,0.9]，结果显示 0.3 与 0.5 效果差不多，再从 [0.3,0.35,0.4,0.45,0.5] 中做一次选择，看不出什么区别，暂定为 0.3  
        - [x] 使用 ppo 在 pybullet 中的 HumanoidFlagrunBulletEnv-v0 训练 2000 个 epochs，参数如下：  
              隐藏网络 [16,16]，target_kl=0.07，clip_ratio=0.3，8个不同的随机种子，结果如下：  
              模型在大约 2000000 steps 即 500 epochs 后收敛，最终效果还是很烂，机器人根本站不稳，所有的 epoch 都是一开始就结束；这种情况有两种可能的原因：  
              1. 环境本身设置的不合理，尤其是结束条件(done),以及奖励等  
              2. 算法使用的不到位，网络大小搞大一点试试  
        - [x] 改为在 HopperBulletEnv-v0 上用 ppo 训练，最终得到了一个不错的策略，ppo 的训练是有一定效果的，可能只是 HumanoidFlagrunBulletEnv-v0 这个环境设计的不好  
        - [ ] 改用 HumanoidBulletEnv-v0 进行训练，
        - [x] 使用三层隐藏层的网络、ppo 与 ddpg 进行训练 [[32,16,8],[32,16,16],[32,32,16],[64,32,16]]，结果并没有什么差别，暂时判断原因为：  
              1. 奖励设置不合理，将动作惩罚系数设置过高，使得模型最终选择静止来减少惩罚  
              2. 探索不足导致模型过快收敛于基本静止的状态  
        - [ ] 载入机器人模型并进行操控  

使用 spinup 训练 Centaur-v0 时， steps_per_epoch 小于4000时会报错  
使用 ppo 训练 Pendulum 时，如果设置 steps_per_epoch 小于默认值(4000)时会出现截断现象，此处必有蹊跷  
使用 ddpg 训练 Pendulum 时，如果设置 steps_per_epoch 小于默认值(4000)时会报错；  
ppo、ddpg 同样使用默认参数训练 Pendulum，ddpg 每个 epoch 用时远多于 ppo (约10倍)，最终效果同样是 ddpg 有效果，而 ppo 毫无效果，这部分原因可能是：ddpg 是 off-policy，而 ppo 是 on-policy，这一点导致了速度的不同；  
关于 ppo 在 Pendulum 无效的原因是：网络大小导致的，使用 [8,8] 或 [256,256] 的网络都会让 ppo 最终无效，修改网络大小后效果如下：  
   1. 收敛且稳定(3个随机种子方差小)：[32,16],[32,8],[64,16],[8,4]  
   2. 不稳定，可能收敛(3个种子中有的收敛)：[16,16],[16,8]  
   3. 不能得到好的策略：[128,64],[256,256]  
ddpg 训练完 Pendulum 后，按照图像看其效果达到了 -200，然而使用 test_policy 时却表现的如同没有训练过一样，平均回报仅在 -1550，此处也有蹊跷；可能是随机种子的问题，seed=0时出现的情况在 seed=10,20 时就没有了  
[官方文档提到(第一个 You Should Know)](https://spinningup.openai.com/en/latest/user/saving_and_loading.html)：spinningup 目前不能恢复部分训练的模型继续训练，所以训练只能一次性完成  
使用带不同隐藏层的 ppo、ddpg 训练 Flagrun ，都没有像样的效果，暂时定论为环境本身设置不合理，暂时搁置该环境：  

## 2015.5.17

- [ ] 完整分析 HumanoidBulletEnv-v0 环境，为后面写自己的环境作参考  
   - [ ] 分析行走机器类 WalkerBase：  
   - [x] 分析MuJoCo机器人基类 MJCFBasedRobot  
   - [x] 分析机器人基类 XmlBasedRobot  
   - [ ] 分析环境基类 MJCFBasedBulletEnv  
   - [ ] 分析行走机器环境类 WalkerBaseBulletEnv  
   - [ ] 分析人形行走机器环境类 HumanoidBulletEnv  
   - [ ] 整理所有类，给出环境的流程框架  


## 2020.5.18      pybullet 接口功能说明

setJointMotorControl2:  
      控制结构体运动，包括 POSITION_CONTROL, VELOCITY_CONTROL, TORQUE_CONTROL and PD_CONTROL.  
getBasePositionAndOrientation：  
      获得 base 的位置和姿态信息，返回为 x,y,z,w 其中 w 为姿态信息(4维)  
getLinkState:  
      获得 link 质心的位置、姿态、线速度、角速度等信息  
getBaseVelocity,resetBaseVelocity：
      获得/重置速度，速度包括线速度与角速度  
resetBasePositionAndOrientation：  
      重置 base 的位置与姿态  
resetBaseVelocity：  
      重置 base 的速度  
getContactPoint：  
      获取部件关节的连接信息，包括连接点、连接距离等信息  
getJointInfo:  
      关节信息，包括关节名、关节类型、转动限制、速度限制等信息  
resetJointState：  
      